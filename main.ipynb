{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook - training the model and outputting midi files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing functions / tf packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from func.jupyter_tensorboard import show_graph\n",
    "from func.midi_to_statematrix import *\n",
    "from func.data import *\n",
    "import os\n",
    "import pickle\n",
    "import signal\n",
    "import numpy as np\n",
    "import random\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to save and import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    WD = os.getcwd()\n",
    "    with open(WD +'/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_obj(name):\n",
    "    WD = os.getcwd()\n",
    "    with open(WD + '/'+ name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From midi to inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the batch width (number of sequences in a batch), the batch length (length of each sequence) and the interval between possible start locations. Then, We use Johnson's functions (loadPieces getPieceSegment and getPieceBatch) to transform a midi file in an input ready to be trained on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_width = 10 # number of sequences in a batch\n",
    "batch_len = 16*8 # length of each sequence\n",
    "division_len = 16 # interval between possible start locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPieces(dirpath):\n",
    "\n",
    "    pieces = {}\n",
    "\n",
    "    for fname in os.listdir(dirpath):\n",
    "        if fname[-4:] not in ('.mid','.MID'):\n",
    "            continue\n",
    "\n",
    "        name = fname[:-4]\n",
    "\n",
    "        outMatrix = midiToNoteStateMatrix(os.path.join(dirpath, fname))\n",
    "        if len(outMatrix) < batch_len:\n",
    "            continue\n",
    "\n",
    "        pieces[name] = outMatrix\n",
    "        print(\"Loaded {}\".format(name))\n",
    "\n",
    "    return pieces\n",
    "\n",
    "def getPieceSegment(pieces):\n",
    "    pcs=pieces.values()\n",
    "    piece_output = random.choice(list(pcs))\n",
    "    start = random.randrange(0,len(piece_output)-batch_len,division_len)\n",
    "    \n",
    "    # print \"Range is {} {} {} -> {}\".format(0,len(piece_output)-batch_len,division_len, start)\n",
    "\n",
    "    seg_out = piece_output[start:start+batch_len]\n",
    "    seg_in = noteStateMatrixToInputForm(seg_out)\n",
    "\n",
    "    return seg_in, seg_out\n",
    "\n",
    "def getPieceBatch(pieces):\n",
    "    i,o = zip(*[getPieceSegment(pieces) for _ in range(batch_width)])\n",
    "    return numpy.array(i), numpy.array(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the midi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded alb_esp2\n",
      "Loaded alb_esp5\n",
      "Loaded appass_2\n",
      "Loaded appass_3\n",
      "Loaded bach_846\n",
      "Loaded bach_847\n",
      "Loaded bach_850\n",
      "Loaded beethoven_hammerklavier_1\n",
      "Loaded beethoven_les_adieux_1\n",
      "Loaded beethoven_les_adieux_2\n",
      "Loaded beethoven_opus10_2\n",
      "Loaded beethoven_opus10_3\n",
      "Loaded beethoven_opus22_1\n",
      "Loaded beethoven_opus22_4\n",
      "Loaded beethoven_opus90_2\n"
     ]
    }
   ],
   "source": [
    "pcs = loadPieces(path  + '/music_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model architecture through the Model function. We also define other subfunctions to set the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We choose one file to train the model on\n",
    "song={}\n",
    "song['beethoven_hammerklavier_1']=pcs['beethoven_hammerklavier_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model(t_layer_sizes,p_layer_sizes,xs,ys):\n",
    "\n",
    "    \n",
    "    #xs\n",
    "    input_slice = xs[:,0:-1]\n",
    "    \n",
    "    n_batch, n_time, n_note, n_ipn = tf.shape(input_slice)[0], tf.shape(input_slice)[1],tf.shape(input_slice)[2],input_slice.get_shape().as_list()[3]\n",
    "    \n",
    "    input_slice = tf.reshape(tf.transpose(input_slice,(1,0,2,3)),(n_time,n_batch*n_note,n_ipn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Defining the first lstm \n",
    "    \n",
    "    t_input_size = 80\n",
    "    lstm_time=[]\n",
    "    for i in t_layer_sizes:\n",
    "        lstm_time.append(tf.contrib.rnn.LSTMCell(i))\n",
    "\n",
    "    time_model=tf.contrib.rnn.MultiRNNCell(lstm_time)        \n",
    "    init_state_time=time_model.zero_state(tf.shape(ys)[0],tf.float32)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('lstm1'):\n",
    "        #tf.get_variable\n",
    "        outputs_time,final_state_time=tf.nn.dynamic_rnn(time_model, input_slice, dtype = tf.float32)\n",
    "        for one_lstm_cell in lstm_time:\n",
    "            one_kernel, one_bias = one_lstm_cell.variables\n",
    "            # I think TensorBoard handles summaries with the same name fine.\n",
    "            tf.summary.histogram(\"Kernel-time\", one_kernel)\n",
    "            tf.summary.histogram(\"Bias-time\", one_bias)    \n",
    "    p_input_size = t_layer_sizes[-1] + 2\n",
    "\n",
    "    # Transpose to be (note, batch/time, hidden_states)\n",
    "    n_hidden = outputs_time.get_shape().as_list()[2]\n",
    "   \n",
    "    time_final = tf.reshape(tf.transpose(tf.reshape(outputs_time,(n_time,n_batch,n_note,n_hidden)),(2,1,0,3)),(n_note,n_batch*n_time,n_hidden))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    start_note_values = tf.zeros([1,tf.shape(time_final)[1] ,2], tf.float32)\n",
    "    correct_choices = tf.reshape(tf.transpose(ys[:,1:,0:-1,:],(2,0,1,3)),(n_note-1,n_batch*n_time,2))\n",
    "    note_choices_inputs = tf.concat([start_note_values, correct_choices], axis=0)    \n",
    "    note_inputs = tf.concat( [time_final, note_choices_inputs], axis=2)\n",
    "    num_timebatch = note_inputs.shape[1]\n",
    "\n",
    "    #Defining the second lstm\n",
    "    \n",
    "    lstm_pitch=[]\n",
    "\n",
    "    for i in p_layer_sizes:\n",
    "        lstm_pitch.append(tf.contrib.rnn.LSTMCell(i))\n",
    "    lstm_pitch.append(tf.contrib.rnn.LSTMCell(2))\n",
    "\n",
    "\n",
    "    pitch_model=tf.contrib.rnn.MultiRNNCell(lstm_pitch)\n",
    "    \n",
    "    init_state_pitch=pitch_model.zero_state(tf.shape(note_inputs)[0],tf.float32)\n",
    "    with tf.variable_scope('lstm2'):\n",
    "        outputs_pitch,final_state_pitch=tf.nn.dynamic_rnn(pitch_model,note_inputs,dtype = tf.float32)\n",
    "        for one_lstm_cell in lstm_pitch:\n",
    "            one_kernel, one_bias = one_lstm_cell.variables\n",
    "            \n",
    "            \n",
    "            tf.summary.histogram(\"Kernel-pitch\", one_kernel)\n",
    "            tf.summary.histogram(\"Bias-pitch\", one_bias)    \n",
    "     \n",
    "    outputs_pitch = tf.transpose(tf.reshape(outputs_pitch,(n_note,n_batch,n_time,2)),(1,2,0,3)) \n",
    " \n",
    "    \n",
    "    #Defining the cost function with tf operations\n",
    "    \n",
    "    term_1=tf.multiply(2.0,tf.multiply(tf.sigmoid(outputs_pitch),ys[:,1:]))\n",
    "    term_2=tf.multiply(-1.0,tf.sigmoid(outputs_pitch))\n",
    "    term_3=tf.multiply(-1.0,ys[:,1:])\n",
    "    \n",
    "    term_4=tf.add(term_1,term_2)\n",
    "    term_5=tf.add(term_4,term_3)\n",
    "    term_6=tf.add(1.0,term_5)\n",
    "    \n",
    "    term_7=tf.log(term_6)\n",
    "    term_8=tf.reduce_mean(term_7)\n",
    "    \n",
    "    term_9=tf.multiply(-1.0,term_8)\n",
    "    \n",
    "    return (outputs_pitch,term_9)\n",
    "\n",
    "\n",
    "def cross_entropy(output, input_y):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "\n",
    "        ce = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=input_y[:,1:], logits=output))\n",
    "        \n",
    "    return ce\n",
    "\n",
    "\n",
    "def train_step(loss, learning_rate=1e-3):\n",
    "    with tf.name_scope('train_step'):\n",
    "        step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    return step\n",
    "\n",
    "\n",
    "def evaluate(output, input_y):\n",
    "    with tf.name_scope('evaluate'):\n",
    "        pred = tf.argmax(output, axis=1)\n",
    "        error_num = tf.count_nonzero(pred - tf.cast(input_y, tf.int64), name='error_num')\n",
    "        \n",
    "    return error_num\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the architecture is well-defined, we can start training the model through the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(song,t_layer_sizes,p_layer_sizes, pre_trained_model=None):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # Define the variables and parameter needed during training\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(tf.float32, [None,None,None, t_input_size])\n",
    "        ys = tf.placeholder(tf.float32, [None,None,None, 2])\n",
    "    m= Model(t_layer_sizes,p_layer_sizes,xs,ys) \n",
    "    output= m[0]\n",
    "    \n",
    "    \n",
    "    loss=m[1]\n",
    "    \n",
    "    \n",
    "    iters = int(np.array(list(song.values())[0]).shape[0] / batch_len)\n",
    "    \n",
    "    print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "    step = train_step(loss)\n",
    "    eve = evaluate(output, ys)\n",
    "\n",
    "    iter_total = 0\n",
    "    best_acc = 0\n",
    "    cur_model_name = 'amadeus'\n",
    "\n",
    "    epoch=1\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        merge = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "        # try to restore the pre_trained\n",
    "        if pre_trained_model is not None:\n",
    "            try:\n",
    "                print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "                saver.restore(sess, 'model_save/{}'.format(pre_trained_model))\n",
    "            except Exception:\n",
    "                print(\"Load model Failed!\")\n",
    "                pass\n",
    "\n",
    "        for epc in range(epoch):\n",
    "            print(\"epoch {} \".format(epc + 1))\n",
    "\n",
    "            for itr in range(iters):\n",
    "\n",
    "                training_batch_x,training_batch_y= map(numpy.array, getPieceBatch(song))\n",
    "                \n",
    "                _, cur_loss = sess.run([step, loss], feed_dict={xs: training_batch_x, ys:training_batch_y})\n",
    "                    \n",
    "                merge_result=sess.run(merge, feed_dict={xs: training_batch_x, ys: training_batch_y})\n",
    "\n",
    "                saver.save(sess,'model_save/{}'.format(cur_model_name))\n",
    "\n",
    "                train_writer.add_summary(merge_result,itr)\n",
    "               \n",
    "                \n",
    "                print(cur_loss)\n",
    "                \n",
    "\n",
    "                    \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=training(song,[300,300],[100,50], pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results of our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the graph\n",
    "from func.jupyter_tensorboard import show_graph \n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model_save/amadeus.meta')\n",
    "    graph = tf.get_default_graph()\n",
    "    show_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model along with its parameters, we can use it to generate prediction probabilities through the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(song,t_layer_sizes,p_layer_sizes, pre_trained_model=None):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # define the variables and parameter needed during training\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(tf.float32, [None,None,None, t_input_size])\n",
    "        ys = tf.placeholder(tf.float32, [None,None,None, 2])\n",
    "    m= Model(t_layer_sizes,p_layer_sizes,xs,ys) \n",
    "    output= m[0]\n",
    "    \n",
    "    cur_model_name = 'amadeus'\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        xIpt, xOpt = map(lambda x: numpy.array(x, dtype='int8'), getPieceSegment(song))                \n",
    "        saver = tf.train.import_meta_graph('model_save/amadeus.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('model_save/'))\n",
    "                \n",
    "        feed_dict= {xs : xIpt.reshape([1,128,78,80]), ys : xOpt.reshape([1,128,78,2]) }\n",
    "        predi=sess.run([output],feed_dict)\n",
    "        return predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_input_size=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_save/amadeus\n"
     ]
    }
   ],
   "source": [
    "a=prediction(song,[300,300],[100,50], pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From probabilities to binary outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a few functions to transform the probabilies into binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transnote(x):\n",
    "    x[:,0]/=x[:,0].max()\n",
    "    b=np.percentile(x[:,1],70)\n",
    "    for i in range(len(x)):\n",
    "        if (x[i][0]==1):\n",
    "            x[i][0]=1\n",
    "            x[i][1]=1*(x[i][1]>b)\n",
    "            \n",
    "        else : \n",
    "            x[i][0]=0\n",
    "            x[i][1]=0\n",
    "    return x\n",
    "def superpiece(x):\n",
    "    piece=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        piece.append(transnote(a[0][0][i]))\n",
    "        \n",
    "    return piece\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From output to midi file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noteStateMatrixToMidi(np.array(superpiece(a[0][0])), name=\"output\"+str('bonus'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
